{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## muP on Huggingface demo on Translation model with IWSLT En-Ko corpus & T5 structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### requirements:\n",
    "* numpy>=1.18.5\n",
    "* pandas>=1.1.2\n",
    "* torch>=1.6.0\n",
    "* torchvision>=0.7.0\n",
    "* seaborn>=0.11.2\n",
    "* transformers>=4.16.2\n",
    "* pyyaml\n",
    "* sacrebleu\n",
    "* sentencepiece\n",
    "* mup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "\n",
    "import transformers\n",
    "from transformers import MT5Tokenizer\n",
    "from transformers import T5Config, T5ForConditionalGeneration\n",
    "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from transformers import SchedulerType, get_scheduler\n",
    "\n",
    "from datasets import load_dataset, load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed for reproducibility\n",
    "SEED = 890112\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prepare IWSLT en-ko corpus with HF datsets.\n",
    "### you probably want to change DATA_DIR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/path/to/data'\n",
    "\n",
    "# load the raw dataset\n",
    "raw_dataset = load_dataset(\"iwslt2017\", \"iwslt2017-en-ko\", cache_dir=DATA_DIR)\n",
    "\n",
    "# prepare the prefix for pre-processing\n",
    "source_lang = \"en\"\n",
    "target_lang = \"ko\"\n",
    "prefix = \"translate English to Korean: \"\n",
    "\n",
    "# load the pre-trained tokenizer; we use pre-trained mT5 tokenizer, yet we still train the model from scratch.\n",
    "tokenizer = MT5Tokenizer.from_pretrained(\"google/mt5-base\", model_max_length=128)\n",
    "\n",
    "# define preprocess function that adds the prefix on decoder-side and tokenizes the dataset\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + example[source_lang] for example in examples[\"translation\"]]\n",
    "    targets = [example[target_lang] for example in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=128, truncation=True)\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=128, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# tokenize dataset\n",
    "tokenized_dataset = raw_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Set configs with different widths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# narrow model\n",
    "narrow_config = T5Config.from_pretrained(\"t5-small\")\n",
    "narrow_config.d_ff = 1024\n",
    "narrow_config.d_kv = 32\n",
    "narrow_config.d_model = 256\n",
    "narrow_config.num_heads = 8\n",
    "narrow_config.vocab_size = tokenizer.vocab_size\n",
    "\n",
    "# wide model\n",
    "wide_config = T5Config.from_pretrained(\"t5-small\")\n",
    "wide_config.d_ff = 3072\n",
    "wide_config.d_kv = 64\n",
    "wide_config.d_model = 768\n",
    "wide_config.num_heads = 12\n",
    "wide_config.vocab_size = tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post processing & metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"sacrebleu\")\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model and Train \n",
    "### you probably need to restart this ipynb to train several models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model \n",
    "model = T5ForConditionalGeneration(config=narrow_config) # narrow model\n",
    "OUT_DIR = './results/SP-narrow' # narrow model\n",
    "\n",
    "# model = T5ForConditionalGeneration(config=wide_config) # wide model\n",
    "# OUT_DIR = './results/SP-wide' # wide model\n",
    "print(\"model size: %d\" % sum([p.numel() for p in model.parameters()]))\n",
    "\n",
    "# build optimizer and constant lr scheduler\n",
    "optimizer = AdamW(params=model.parameters(), lr=1e-3, weight_decay=0.01)\n",
    "lr_scheduler = get_scheduler(name=SchedulerType.CONSTANT_WITH_WARMUP, optimizer=optimizer, num_warmup_steps=500)\n",
    "\n",
    "# load data collator; HF trainer needs this.\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "# set batch_size infos; default batch_size=64\n",
    "total_batch_size=64\n",
    "num_gpus = torch.cuda.device_count()\n",
    "per_device_batch_size = total_batch_size // num_gpus\n",
    "\n",
    "# HF Trainer args\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results/SP-wide\",\n",
    "    evaluation_strategy=\"steps\",    \n",
    "    eval_steps=2000,\n",
    "    per_device_train_batch_size=per_device_batch_size,\n",
    "    per_device_eval_batch_size=per_device_batch_size,    \n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "# build HF Trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    optimizers=[optimizer, lr_scheduler],\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
